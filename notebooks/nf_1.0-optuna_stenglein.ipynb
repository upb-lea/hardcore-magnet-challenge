{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Least Squares on magnet challenge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.integrate import trapezoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/nikolasf/Dokumente/01_git/30_Python/MC_UPB/data/input/processed'\n",
    "material_name = 'ten_materials'\n",
    "ds = pd.read_pickle(f'{filepath}/{material_name}.pkl.gz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial edit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           2319.444340\n",
      "1           3191.235893\n",
      "2           4341.086142\n",
      "3           5795.359190\n",
      "4           7813.691725\n",
      "              ...      \n",
      "186742     68121.464932\n",
      "186743     85924.743654\n",
      "186744    108411.839417\n",
      "186745    135853.384296\n",
      "186746    171849.522231\n",
      "Name: ploss, Length: 186747, dtype: float64\n",
      "0          7.749083\n",
      "1          8.068164\n",
      "2          8.375880\n",
      "3          8.664813\n",
      "4          8.963633\n",
      "            ...    \n",
      "186742    11.129048\n",
      "186743    11.361227\n",
      "186744    11.593693\n",
      "186745    11.819332\n",
      "186746    12.054375\n",
      "Name: ploss, Length: 186747, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ds = ds.drop(columns=[c for c in ds if c.startswith(\"H_t\")])\n",
    "#ds = ds.query('temp == 25')\n",
    "\n",
    "# add the saturation flux density. Data from datasheets.\n",
    "ds.loc[ds['material'] == '3C90', 'b_sat_25'] = 0.47\n",
    "ds.loc[ds['material'] == '3C94', 'b_sat_25'] = 0.47\n",
    "ds.loc[ds['material'] == '3E6', 'b_sat_25'] = 0.46\n",
    "ds.loc[ds['material'] == '3F4', 'b_sat_25'] = 0.41\n",
    "ds.loc[ds['material'] == '77', 'b_sat_25'] = 0.51\n",
    "ds.loc[ds['material'] == '78', 'b_sat_25'] = 0.48\n",
    "ds.loc[ds['material'] == 'N27', 'b_sat_25'] = 0.50\n",
    "ds.loc[ds['material'] == 'N30', 'b_sat_25'] = 0.38\n",
    "ds.loc[ds['material'] == 'N49', 'b_sat_25'] = 0.49\n",
    "ds.loc[ds['material'] == 'N87', 'b_sat_25'] = 0.49\n",
    "\n",
    "\n",
    "print(ds['ploss'])\n",
    "ds['ploss'] = np.log(ds['ploss'])\n",
    "ds['freq'] = np.log(ds['freq'])\n",
    "\n",
    "print(ds['ploss'])\n",
    "\n",
    "# print(ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapted Wilhelm example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run linear regression with regularization training\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor, VotingRegressor, ExtraTreesRegressor\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from utils.experiments import get_stratified_fold_indices, PROC_SOURCE\n",
    "from utils.metrics import calculate_metrics\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    n_estimators = 429 # trial.suggest_int('n_estimators', 10, 500)\n",
    "    print(f'{n_estimators = }')\n",
    "    criterion = trial.suggest_categorical('criterion', ['squared_error'])\n",
    "    print(f\"{criterion = }\")\n",
    "    learning_rate = 0.12 # trial.suggest_float('learning_rate', 0.01, 1)\n",
    "    print(f\"{learning_rate = }\")\n",
    "    \n",
    "    exp_log = {}\n",
    "    for material_lbl, mat_df in tqdm(ds.groupby(\"material\"), desc=\"Train across materials\"):\n",
    "        full_b = mat_df.loc[:, [f\"B_t_{k}\" for k in range(1024)]].to_numpy()\n",
    "        dbdt = full_b[:, 1:] - full_b[:, :-1]\n",
    "        mat_df = mat_df.reset_index(drop=True)\n",
    "\n",
    "        x_vec = np.linspace(0, 1023, 1024)\n",
    "        b_vec = []\n",
    "        for value in x_vec:\n",
    "            b_vec.append(f'B_t_{int(value)}')\n",
    "        mat_df[\"b\"] = mat_df[b_vec].values.tolist()\n",
    "        x_vec = None\n",
    "        b_vec = None\n",
    "        mat_df['delta_b'] = mat_df['b'].map(lambda x: np.max(x) - np.min(x))\n",
    "\n",
    "        # figure out integral_part \n",
    "        mat_df[\"time_s\"] = mat_df[\"freq\"].map(lambda x: np.linspace(0, 1/x, 1024))\n",
    "\n",
    "        # derivation\n",
    "        # according to https://im-coder.com/zweite-ableitung-in-python-scipy-numpy-pandas.html\n",
    "        mat_df[\"fitted_function\"] = mat_df.apply(lambda x: UnivariateSpline(x[\"time_s\"], x[\"b\"], s=0, k=4), axis=1)\n",
    "        mat_df[\"amplitude_2nd_derivation\"] = mat_df[\"fitted_function\"].apply(lambda x: x.derivative(n=2))\n",
    "        mat_df[\"integrated_function\"] = mat_df.apply(lambda x: trapezoid(np.abs(x[\"amplitude_2nd_derivation\"](x[\"time_s\"])), x[\"time_s\"]), axis=1)     \n",
    "\n",
    "        mat_df[\"integral_part\"] = mat_df[\"integrated_function\"] / mat_df[\"delta_b\"]\n",
    "\n",
    "        # cross validation 'kfold'\n",
    "        kfold_lbls = get_stratified_fold_indices(mat_df, 4)\n",
    "        mat_df_proc = mat_df.assign(\n",
    "            kfold=kfold_lbls,\n",
    "            # integral_part=mat_df['integral_part'],\n",
    "            #delta_b=mat_df['delta_b'],\n",
    "            #b_sat=mat_df['b_sat_25'],\n",
    "            db_bsat_1 = mat_df['delta_b'] / mat_df['b_sat_25'],\n",
    "            db_bsat_2 = (mat_df['delta_b'] / mat_df['b_sat_25']) ** 2,\n",
    "            db_bsat_3 = (mat_df['delta_b'] / mat_df['b_sat_25']) ** 3,\n",
    "            db_bsat_4 = (mat_df['delta_b'] / mat_df['b_sat_25']) ** 4,\n",
    "            db_bsat_5 = (mat_df['delta_b'] / mat_df['b_sat_25']) ** 5,\n",
    "            db_bsat_6 = (mat_df['delta_b'] / mat_df['b_sat_25']) ** 6,\n",
    "            temp_square = mat_df['temp'] ** 2,\n",
    "            t2 = (mat_df['integral_part'] ** (-1)) / 2,\n",
    "            t3 = -(mat_df['integral_part'] ** (-2)) / 6,\n",
    "            t4 = ( 3 * mat_df['integral_part'] ** (-2) + 2 * mat_df['integral_part'] ** (-3)) / 24\n",
    "            # more features imaginable (count of spikes e.g.)\n",
    "        ).drop(\n",
    "            columns=[c for c in mat_df if c.startswith(\"B_t_\")] + [\"material\"] + ['b'] + ['time_s'] + ['fitted_function'] +['amplitude_2nd_derivation'] + ['integrated_function'] + ['b_sat_25']\n",
    "        )  # drop B curve\n",
    "\n",
    "        # training result container\n",
    "        results_df = mat_df_proc.loc[:, [\"ploss\", \"kfold\"]].assign(pred=0)\n",
    "        x_cols = [c for c in mat_df_proc if c not in [\"ploss\", \"kfold\"]]\n",
    "        print(x_cols)\n",
    "        for kfold_lbl, test_fold_df in mat_df_proc.groupby(\"kfold\"):\n",
    "            train_fold_df = (\n",
    "                mat_df_proc.query(\"kfold != @kfold_lbl\")\n",
    "                .reset_index(drop=True)\n",
    "                .drop(columns=\"kfold\")\n",
    "            )\n",
    "            assert len(train_fold_df) > 0, \"empty dataframe error\"\n",
    "            y = train_fold_df.pop(\"ploss\")\n",
    "            X = train_fold_df.loc[:, x_cols]\n",
    "\n",
    "            mdl = GradientBoostingRegressor(n_estimators=n_estimators, criterion=criterion, learning_rate=learning_rate) # GradientBoostingRegressor() # HistGradientBoostingRegressor() # RandomForestRegressor(n_estimators = 100) #LinearRegression() # Ridge()  # \n",
    "            mdl.fit(X.to_numpy(), y.to_numpy())\n",
    "            pred = mdl.predict(test_fold_df.loc[:, x_cols].to_numpy())\n",
    "            results_df.loc[results_df.kfold == kfold_lbl, \"pred\"] = pred\n",
    "\n",
    "        # book keeping\n",
    "        exp_log[material_lbl] = calculate_metrics(\n",
    "            np.exp(results_df.loc[:, \"pred\"]), np.exp(results_df.loc[:, \"ploss\"])\n",
    "        )\n",
    "    print(mdl.__class__.__name__)\n",
    "    print(f\"{n_estimators = }\")\n",
    "    print(f\"{learning_rate = }\")\n",
    "    print(mdl.feature_importances_)\n",
    "    \n",
    "    print(\"Overall Score\")\n",
    "    score = pd.DataFrame(exp_log).T\n",
    "    print(score)\n",
    "\n",
    "    added_avg_abs_rel_err = score['avg-abs-rel-err'].sum()\n",
    "    print('sum error')\n",
    "    print(added_avg_abs_rel_err)\n",
    "    \n",
    "    # feature importances\n",
    "    feat_importances = pd.Series(mdl.feature_importances_, index=x_cols)\n",
    "    feat_importances.nlargest(20).plot(kind='barh')\n",
    "    \n",
    "    return added_avg_abs_rel_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mdl.features_importances_)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run objective function single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run objective function in a hyperparameter optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-01 18:11:36,970] A new study created in memory with name: no-name-76c7598b-adc2-402d-bd85-f057c6a784af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 429\n",
      "criterion = 'squared_error'\n",
      "learning_rate = 0.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train across materials:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['freq', 'temp', 'delta_b', 'integral_part', 'db_bsat_1', 'db_bsat_2', 'db_bsat_3', 'db_bsat_4', 'db_bsat_5', 'db_bsat_6', 'temp_square', 't2', 't3', 't4']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# FYI: Objective functions can take additional arguments\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\u001b[39;00m\n\u001b[1;32m     14\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m     16\u001b[0m \u001b[39m#print(study.best_trial)\u001b[39;00m\n",
      "File \u001b[0;32m~/Dokumente/01_git/30_Python/MC_UPB/.venv/lib/python3.10/site-packages/optuna/study/study.py:443\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    340\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    341\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    350\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \n\u001b[1;32m    352\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     _optimize(\n\u001b[1;32m    444\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    445\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    446\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    447\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    448\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    449\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    450\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    451\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    452\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    453\u001b[0m     )\n",
      "File \u001b[0;32m~/Dokumente/01_git/30_Python/MC_UPB/.venv/lib/python3.10/site-packages/optuna/study/_optimize.py:85\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     82\u001b[0m time_start \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m     83\u001b[0m futures: Set[Future] \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m---> 85\u001b[0m \u001b[39mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[39m=\u001b[39mn_jobs) \u001b[39mas\u001b[39;00m executor:\n\u001b[1;32m     86\u001b[0m     \u001b[39mfor\u001b[39;00m n_submitted_trials \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mcount():\n\u001b[1;32m     87\u001b[0m         \u001b[39mif\u001b[39;00m study\u001b[39m.\u001b[39m_stop_flag:\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshutdown(wait\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    650\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         t\u001b[39m.\u001b[39;49mjoin()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mcannot join current thread\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_tstate_lock()\n\u001b[1;32m   1097\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[39m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[39m=\u001b[39m\u001b[39mmax\u001b[39m(timeout, \u001b[39m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[39mif\u001b[39;00m lock\u001b[39m.\u001b[39;49macquire(block, timeout):\n\u001b[1;32m   1117\u001b[0m         lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "\n",
    "import sklearn.datasets\n",
    "import sklearn.ensemble\n",
    "import sklearn.model_selection\n",
    "import sklearn.svm\n",
    "\n",
    "\n",
    "# FYI: Objective functions can take additional arguments\n",
    "# (https://optuna.readthedocs.io/en/stable/faq.html#objective-func-additional-args).\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=1, n_jobs=2)\n",
    "#print(study.best_trial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "862f57f7b4a9bcb8e62a2c8b5a26d37db35f8703e11463ebdedd88c83882dd1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
