{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oli\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import optuna\n",
    "import numpy as np\n",
    "import xgboost\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "from utils.experiments import get_stratified_fold_indices, PROC_SOURCE, form_factor, crest_factor\n",
    "from utils.metrics import calculate_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    ds = pd.read_pickle(PROC_SOURCE / \"ten_materials.pkl.gz\")\n",
    "    #hyperparameters\n",
    "    max_depth= trial.suggest_int(\"max_depth\",9,12,log=True)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\",400,1000)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.05,0.15)\n",
    "\n",
    "    # drop H curve, we only take power loss as target\n",
    "    ds = ds.drop(columns=[c for c in ds if c.startswith(\"H_t\")])\n",
    "    exp_log = {}\n",
    "    feature_imp_sum= np.zeros(10)\n",
    "    for material_lbl, mat_df in tqdm(\n",
    "        ds.groupby(\"material\"), desc=\"Train across materials\"\n",
    "    ):\n",
    "        full_b = mat_df.loc[:, [f\"B_t_{k}\" for k in range(1024)]].to_numpy()\n",
    "        dbdt = full_b[:, 1:] - full_b[:, :-1]\n",
    "        mat_df = mat_df.reset_index(drop=True)\n",
    "        kfold_lbls = get_stratified_fold_indices(mat_df, 4)\n",
    "        mat_df_proc = mat_df.assign(\n",
    "            kfold=kfold_lbls,\n",
    "            log_freq= np.log10(mat_df.loc[:,'freq']),\n",
    "            #b_fft=np.fft.fft(full_b),\n",
    "            b_fft_mean=np.mean(np.abs(np.fft.fft(full_b)),axis=1),\n",
    "            b_peak2peak=full_b.max(axis=1) - full_b.min(axis=1),\n",
    "            log_peak2peak = np.log10(full_b.max(axis=1) - full_b.min(axis=1)),\n",
    "            max_dbdt=np.max(dbdt, axis=1),\n",
    "            min_dbdt=np.min(dbdt, axis=1),\n",
    "            mean_abs_dbdt=np.mean(np.abs(dbdt), axis=1),\n",
    "            #crest_fac=crest_factor(full_b),\n",
    "            form_fac=form_factor(full_b)\n",
    "            # median_dbdt=np.median(dbdt, axis=1)\n",
    "            # more features imaginable (count of spikes e.g.)\n",
    "        ).drop(\n",
    "            columns=[c for c in mat_df if c.startswith(\"B_t_\")] + [\"material\"]\n",
    "        )  # drop B curve\n",
    "        # training result container\n",
    "        results_df = mat_df_proc.loc[:, [\"ploss\", \"kfold\"]].assign(pred=0)\n",
    "        x_cols = [c for c in mat_df_proc if c not in [\"ploss\", \"kfold\"]]\n",
    "\n",
    "        for kfold_lbl, test_fold_df in mat_df_proc.groupby(\"kfold\"):\n",
    "            train_fold_df = (\n",
    "                mat_df_proc.query(\"kfold != @kfold_lbl\")\n",
    "                .reset_index(drop=True)\n",
    "                .drop(columns=\"kfold\")\n",
    "            )\n",
    "            assert len(train_fold_df) > 0, \"empty dataframe error\"\n",
    "            y = train_fold_df.pop(\"ploss\")\n",
    "            X = train_fold_df.loc[:, x_cols]\n",
    "\n",
    "            gbm = xgboost.XGBRegressor(max_depth=max_depth, gamma = 0.05822,learning_rate=learning_rate, n_estimators=n_estimators, subsample=0.89965, colsample_bytree=0.76261, objective='reg:squarederror')\n",
    "            gbm.fit(X, y)\n",
    "            pred = gbm.predict(test_fold_df.loc[:, x_cols])\n",
    "            results_df.loc[results_df.kfold == kfold_lbl, \"pred\"] = pred\n",
    "            feature_imp_sum += gbm.feature_importances_\n",
    "            # plot\n",
    "\n",
    "        # book keeping\n",
    "        # print(feature_imp_sum)\n",
    "        # plot\n",
    "        \n",
    "\n",
    "        exp_log[material_lbl] = calculate_metrics(\n",
    "            results_df.loc[:, \"pred\"], results_df.loc[:, \"ploss\"]\n",
    "        )\n",
    "    results=pd.DataFrame(exp_log).T\n",
    "    return np.mean(results.loc[:,\"avg-abs-rel-err\"].to_numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-08 15:38:42,733] A new study created in memory with name: no-name-f3777323-e283-4489-8ef2-4dc6baabfbef\n",
      "Train across materials: 100%|██████████| 10/10 [03:59<00:00, 23.92s/it]\n",
      "[I 2023-08-08 15:42:54,335] Trial 0 finished with value: 0.043035884764537 and parameters: {'max_depth': 9, 'n_estimators': 946, 'learning_rate': 0.10333693807931968}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:16<00:00, 19.61s/it]\n",
      "[I 2023-08-08 15:46:23,516] Trial 1 finished with value: 0.05008844093133533 and parameters: {'max_depth': 12, 'n_estimators': 556, 'learning_rate': 0.13164113745593725}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:39<00:00, 21.94s/it]\n",
      "[I 2023-08-08 15:50:16,313] Trial 2 finished with value: 0.04561040169472873 and parameters: {'max_depth': 11, 'n_estimators': 692, 'learning_rate': 0.09910209601788832}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:36<00:00, 21.64s/it]\n",
      "[I 2023-08-08 15:54:06,113] Trial 3 finished with value: 0.048461849827819516 and parameters: {'max_depth': 12, 'n_estimators': 593, 'learning_rate': 0.09882729738066207}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [01:59<00:00, 11.96s/it]\n",
      "[I 2023-08-08 15:56:19,011] Trial 4 finished with value: 0.04617032630187011 and parameters: {'max_depth': 9, 'n_estimators': 452, 'learning_rate': 0.10049693819025032}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [04:03<00:00, 24.36s/it]\n",
      "[I 2023-08-08 16:00:35,535] Trial 5 finished with value: 0.04723736583732692 and parameters: {'max_depth': 12, 'n_estimators': 686, 'learning_rate': 0.056945711319592265}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [02:04<00:00, 12.48s/it]\n",
      "[I 2023-08-08 16:02:53,502] Trial 6 finished with value: 0.04542149749794105 and parameters: {'max_depth': 10, 'n_estimators': 426, 'learning_rate': 0.08112219747846715}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [05:39<00:00, 33.93s/it]\n",
      "[I 2023-08-08 16:08:45,669] Trial 7 finished with value: 0.04738676016486906 and parameters: {'max_depth': 12, 'n_estimators': 984, 'learning_rate': 0.07712944796544105}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [05:10<00:00, 31.02s/it]\n",
      "[I 2023-08-08 16:14:09,223] Trial 8 finished with value: 0.047325777196560906 and parameters: {'max_depth': 12, 'n_estimators': 896, 'learning_rate': 0.07254096862652513}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:41<00:00, 22.19s/it]\n",
      "[I 2023-08-08 16:18:04,503] Trial 9 finished with value: 0.044624499144646654 and parameters: {'max_depth': 9, 'n_estimators': 908, 'learning_rate': 0.1321136468133417}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:19<00:00, 20.00s/it]\n",
      "[I 2023-08-08 16:21:37,437] Trial 10 finished with value: 0.04400402753958863 and parameters: {'max_depth': 9, 'n_estimators': 812, 'learning_rate': 0.11822141881225331}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:23<00:00, 20.30s/it]\n",
      "[I 2023-08-08 16:25:13,394] Trial 11 finished with value: 0.04381116593693145 and parameters: {'max_depth': 9, 'n_estimators': 818, 'learning_rate': 0.1153891553717554}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:40<00:00, 22.07s/it]\n",
      "[I 2023-08-08 16:29:07,003] Trial 12 finished with value: 0.04600304829268851 and parameters: {'max_depth': 10, 'n_estimators': 803, 'learning_rate': 0.14228856078180574}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:21<00:00, 20.11s/it]\n",
      "[I 2023-08-08 16:32:41,591] Trial 13 finished with value: 0.04395571001164998 and parameters: {'max_depth': 9, 'n_estimators': 814, 'learning_rate': 0.11516841247979608}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [04:33<00:00, 27.38s/it]\n",
      "[I 2023-08-08 16:37:28,157] Trial 14 finished with value: 0.043856470265688 and parameters: {'max_depth': 10, 'n_estimators': 996, 'learning_rate': 0.11237179545639149}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:35<00:00, 21.53s/it]\n",
      "[I 2023-08-08 16:41:17,062] Trial 15 finished with value: 0.04536496210918816 and parameters: {'max_depth': 9, 'n_estimators': 867, 'learning_rate': 0.14754675386609808}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:04<00:00, 18.44s/it]\n",
      "[I 2023-08-08 16:44:34,453] Trial 16 finished with value: 0.043200094506353995 and parameters: {'max_depth': 9, 'n_estimators': 739, 'learning_rate': 0.08925895996398878}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [02:53<00:00, 17.36s/it]\n",
      "[I 2023-08-08 16:47:40,784] Trial 17 finished with value: 0.0440178463994151 and parameters: {'max_depth': 10, 'n_estimators': 615, 'learning_rate': 0.09002424302552339}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [03:50<00:00, 23.03s/it]\n",
      "[I 2023-08-08 16:51:44,139] Trial 18 finished with value: 0.045237313248728775 and parameters: {'max_depth': 11, 'n_estimators': 740, 'learning_rate': 0.08786439220848}. Best is trial 0 with value: 0.043035884764537.\n",
      "Train across materials: 100%|██████████| 10/10 [02:11<00:00, 13.13s/it]\n",
      "[I 2023-08-08 16:54:08,578] Trial 19 finished with value: 0.04478364555879377 and parameters: {'max_depth': 9, 'n_estimators': 512, 'learning_rate': 0.06606588606217598}. Best is trial 0 with value: 0.043035884764537.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=0, state=1, values=[0.043035884764537], datetime_start=datetime.datetime(2023, 8, 8, 15, 38, 42, 734472), datetime_complete=datetime.datetime(2023, 8, 8, 15, 42, 54, 335176), params={'max_depth': 9, 'n_estimators': 946, 'learning_rate': 0.10333693807931968}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'max_depth': IntDistribution(high=12, log=True, low=9, step=1), 'n_estimators': IntDistribution(high=1000, log=False, low=400, step=1), 'learning_rate': FloatDistribution(high=0.15, log=False, low=0.05, step=None)}, trial_id=0, value=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
